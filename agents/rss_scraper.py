"""
RSS Scanner Agent ‚Äî –ü–∞—Ä—Å–∏–Ω–≥ RSS –ª–µ–Ω—Ç —É–∑–±–µ–∫—Å–∫–∏—Ö –°–ú–ò
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç feedparser –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π

–ë–ï–°–ü–õ–ê–¢–ù–û: –ë–µ–∑ –ª–∏–º–∏—Ç–æ–≤
"""

import json
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional

from config import RSS_FEEDS, PAIN_KEYWORDS, BLACKLIST_KEYWORDS, FRESH_DIR, TODAY

# Feedparser
try:
    import feedparser
    FEEDPARSER_AVAILABLE = True
except ImportError:
    FEEDPARSER_AVAILABLE = False
    print("‚ö†Ô∏è feedparser not installed. Run: pip install feedparser")


class RSSScanner:
    """
    –°–∫–∞–Ω–µ—Ä RSS –ª–µ–Ω—Ç –¥–ª—è —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π.
    """
    
    def __init__(self, feeds: List[str]):
        """
        Args:
            feeds: –°–ø–∏—Å–æ–∫ URL RSS –ª–µ–Ω—Ç
        """
        self.feeds = feeds
    
    def fetch_feed(self, url: str) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç –æ–¥–Ω—É RSS –ª–µ–Ω—Ç—É.
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π
        """
        if not FEEDPARSER_AVAILABLE:
            return []
        
        try:
            print(f"üì∞ Fetching: {url}")
            feed = feedparser.parse(url)
            
            if feed.bozo:  # –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞
                print(f"   ‚ö†Ô∏è Parse error: {feed.bozo_exception}")
            
            entries = []
            for entry in feed.entries[:30]:  # –ú–∞–∫—Å 30 –∑–∞–ø–∏—Å–µ–π
                entries.append({
                    "title": entry.get("title", ""),
                    "link": entry.get("link", ""),
                    "summary": entry.get("summary", "")[:500],
                    "published": entry.get("published", ""),
                    "source": feed.feed.get("title", url),
                })
            
            print(f"   ‚úÖ Found {len(entries)} entries")
            return entries
            
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
            return []
    
    def fetch_all_feeds(self) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç –≤—Å–µ RSS –ª–µ–Ω—Ç—ã.
        
        Returns:
            –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π
        """
        all_entries = []
        
        for url in self.feeds:
            entries = self.fetch_feed(url)
            all_entries.extend(entries)
        
        return all_entries


def classify_entry(text: str) -> Dict:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∑–∞–ø–∏—Å—å"""
    text_lower = text.lower()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ blacklist
    for word in BLACKLIST_KEYWORDS:
        if word in text_lower:
            return {"type": "blocked", "score": 0, "keywords": []}
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –±–æ–ª–∏
    pain_score = 0
    matched_keywords = []
    
    for keyword in PAIN_KEYWORDS:
        if keyword in text_lower:
            pain_score += 1
            matched_keywords.append(keyword)
    
    if pain_score >= 2:
        return {"type": "pain", "score": pain_score, "keywords": matched_keywords}
    elif pain_score == 1:
        return {"type": "question", "score": pain_score, "keywords": matched_keywords}
    else:
        return {"type": "neutral", "score": 0, "keywords": []}


def extract_pains_from_entries(entries: List[Dict]) -> List[Dict]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –±–æ–ª–∏ –∏–∑ –∑–∞–ø–∏—Å–µ–π RSS.
    """
    pains = []
    
    for entry in entries:
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º title –∏ summary –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        full_text = f"{entry.get('title', '')} {entry.get('summary', '')}"
        classification = classify_entry(full_text)
        
        if classification["type"] in ["pain", "question"]:
            pains.append({
                "title": entry.get("title", ""),
                "link": entry.get("link", ""),
                "source": entry.get("source", ""),
                "type": classification["type"],
                "score": classification["score"],
                "keywords": classification["keywords"],
            })
    
    return sorted(pains, key=lambda x: x["score"], reverse=True)


def save_rss_data(data: Dict) -> Path:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ RSS"""
    output_dir = FRESH_DIR / "rss"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    output_file = output_dir / f"uz_{TODAY}.json"
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ Saved RSS data to {output_file}")
    return output_file


def run():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–≥–µ–Ω—Ç–∞"""
    print("üì∞ RSS Scanner Agent starting...")
    print(f"üìÖ Date: {TODAY}")
    print(f"üì° Feeds: {len(RSS_FEEDS)}")
    print(f"üîë Feedparser: {'‚úÖ' if FEEDPARSER_AVAILABLE else '‚ùå Not available'}")
    
    scanner = RSSScanner(RSS_FEEDS)
    
    # 1. –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∑–∞–ø–∏—Å–∏
    print("\nüîç Fetching RSS feeds...")
    entries = scanner.fetch_all_feeds()
    
    # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –±–æ–ª–∏
    print("\nüíä Extracting pains...")
    pains = extract_pains_from_entries(entries)
    print(f"   Found {len(pains)} pains")
    
    # 3. –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
    sources = {}
    for entry in entries:
        source = entry.get("source", "Unknown")
        if source not in sources:
            sources[source] = 0
        sources[source] += 1
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    all_data = {
        "date": TODAY,
        "feeds_count": len(RSS_FEEDS),
        "entries_count": len(entries),
        "pains_count": len(pains),
        "sources": sources,
        "entries": entries[:50],  # –¢–æ–ø 50 –∑–∞–ø–∏—Å–µ–π
        "pains": pains[:30],  # –¢–æ–ø 30 –±–æ–ª–µ–π
    }
    
    print(f"\n{'='*50}")
    print(f"üìä Results:")
    print(f"   Feeds parsed: {len(RSS_FEEDS)}")
    print(f"   Total entries: {len(entries)}")
    print(f"   Pains found: {len(pains)}")
    print(f"{'='*50}")
    
    save_rss_data(all_data)
    return all_data


if __name__ == "__main__":
    run()
