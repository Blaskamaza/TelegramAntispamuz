"""
Pain Extractor Agent ‚Äî –ê–Ω–∞–ª–∏–∑ –±–æ–ª–µ–π —Å –ø–æ–º–æ—â—å—é Gemini
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Gemini 3.0 Flash –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ Gemini 3.0 Pro –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.
"""

import json
import time
import os
from datetime import datetime
from pathlib import Path
from collections import Counter
from typing import List, Dict

import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

from config import (
    FRESH_DIR, TODAY, BASE_DIR, GOOGLE_API_KEY,
    GEMINI_PRO_MODEL, GEMINI_FLASH_MODEL, GEMINI_LITE_MODEL,
    GEMINI_RPM, GEMINI_RPD
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Gemini
if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)
else:
    print("‚ö†Ô∏è GOOGLE_API_KEY not found in config/env. AI features will be disabled.")


class RateLimiter:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞–º–∏ API"""
    def __init__(self, rpm: int, rpd: int):
        self.rpm = rpm
        self.rpd = rpd
        self.requests_today = 0
        self.last_request_time = 0
        self.min_interval = 60.0 / rpm

    def wait(self):
        """–û–∂–∏–¥–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º"""
        if self.requests_today >= self.rpd:
            raise Exception("Daily API limit reached!")
        
        now = time.time()
        elapsed = now - self.last_request_time
        
        if elapsed < self.min_interval:
            time.sleep(self.min_interval - elapsed)
        
        self.last_request_time = time.time()
        self.requests_today += 1


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–∏–º–∏—Ç–µ—Ä–∞
limiter = RateLimiter(rpm=GEMINI_RPM, rpd=GEMINI_RPD)


def load_all_fresh_data() -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ data/fresh/"""
    all_data = {
        "trends": [],
        "youtube": [],
        "telegram": [],
        "facebook": [],
        "rss": [],
    }
    
    for source in all_data.keys():
        source_dir = FRESH_DIR / source
        if source_dir.exists():
            for file in source_dir.glob(f"*{TODAY}*.json"):
                try:
                    with open(file, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        all_data[source].append(data)
                except Exception as e:
                    print(f"‚ö†Ô∏è Error loading {file}: {e}")
    
    return all_data


def extract_pain_texts(data: dict) -> list:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —Ç–µ–∫—Å—Ç—ã —Å –±–æ–ª—è–º–∏ –∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
    pain_texts = []
    
    # Trends
    for trends_data in data.get("trends", []):
        for trend in trends_data.get("data", []):
            if trend.get("growth_3m", 0) > 20:
                pain_texts.append({
                    "text": trend.get("keyword", ""),
                    "source": "Google Trends",
                    "score": trend.get("growth_3m", 0),
                })
    
    # YouTube
    for yt_data in data.get("youtube", []):
        for query in yt_data.get("queries", []):
            for video in query.get("videos", []):
                for pain in video.get("top_pains", []):
                    pain_texts.append({
                        "text": pain.get("text", ""),
                        "source": "YouTube",
                        "score": pain.get("score", 0),
                    })
    
    # Telegram
    for tg_data in data.get("telegram", []):
        for channel in tg_data.get("channels", []):
            for post in channel.get("pains", []): # Changed from pain_posts to pains based on tg_scanner
                pain_texts.append({
                    "text": post.get("text", ""),
                    "source": f"Telegram: {channel.get('channel', '')}",
                    "score": post.get("score", 0),
                })
    
    # Facebook
    for fb_data in data.get("facebook", []):
        for group in fb_data.get("groups", []):
            for post in group.get("pain_posts", []):
                pain_texts.append({
                    "text": post.get("text", ""),
                    "source": f"Facebook: {group.get('name', '')}",
                    "score": post.get("pain_score", 0) + post.get("engagement_score", 0) / 10,
                })
    
    # RSS
    for rss_data in data.get("rss", []):
        for feed in rss_data.get("feeds", []):
            for article in feed.get("pain_articles", []):
                pain_texts.append({
                    "text": f"{article.get('title', '')} - {article.get('summary', '')}",
                    "source": "RSS News",
                    "score": article.get("relevance_score", 0),
                })
    
    return pain_texts


def call_gemini(model_name: str, prompt: str) -> str:
    """–í—ã–∑–æ–≤ Gemini API —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –ª–∏–º–∏—Ç–æ–≤"""
    if not GOOGLE_API_KEY:
        return ""
        
    try:
        limiter.wait()
        model = genai.GenerativeModel(model_name)
        response = model.generate_content(
            prompt,
            safety_settings={
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
            }
        )
        return response.text
    except Exception as e:
        print(f"‚ùå Gemini API Error ({model_name}): {e}")
        return ""


def analyze_pains_with_gemini(pain_texts: list) -> list:
    """
    –î–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–æ–ª–µ–π:
    1. Gemini 3.0 Flash: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è (Batching)
    2. Gemini 3.0 Pro: –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–¥–µ–π
    """
    if not pain_texts:
        return []
        
    print(f"   Processing {len(pain_texts)} items...")
    
    # === –≠–¢–ê–ü 1: –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø (Flash) ===
    print("   üöÄ Stage 1: Categorization (Gemini Flash)...")
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –ø–æ 20 —à—Ç—É–∫
    batch_size = 20
    batches = [pain_texts[i:i + batch_size] for i in range(0, len(pain_texts), batch_size)]
    
    categories_data = {}
    
    for i, batch in enumerate(batches):
        print(f"      Processing batch {i+1}/{len(batches)}...")
        
        batch_text = "\n".join([f"- {p['text']} (Source: {p['source']})" for p in batch])
        
        prompt = f"""
        –¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∂–∞–ª–æ–±—ã –∏ –ø—Ä–æ–±–ª–µ–º—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ —Å–ø–∏—Å–∫–∞ –Ω–∏–∂–µ.
        
        –°–ø–∏—Å–æ–∫:
        {batch_text}
        
        –î–ª—è –∫–∞–∂–¥–æ–π –∂–∞–ª–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑ —Å–ø–∏—Å–∫–∞:
        [–†–∞–±–æ—Ç–∞, –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ, –§–∏–Ω–∞–Ω—Å—ã, –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –°–µ–º—å—è, –ñ–∏–ª—å—ë, –ë–∏–∑–Ω–µ—Å, –ü–æ–∫—É–ø–∫–∏, –ó–¥–æ—Ä–æ–≤—å–µ, –î—Ä—É–≥–æ–µ]
        
        –ò–≥–Ω–æ—Ä–∏—Ä—É–π –º—É—Å–æ—Ä, —Å–ø–∞–º –∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è.
        
        –í–µ—Ä–Ω–∏ –æ—Ç–≤–µ—Ç –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
        [
            {{"text": "—Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã", "category": "–ö–∞—Ç–µ–≥–æ—Ä–∏—è", "is_relevant": true/false}}
        ]
        """
        
        response_text = call_gemini(GEMINI_FLASH_MODEL, prompt)
        
        try:
            # –û—á–∏—Å—Ç–∫–∞ markdown –±–ª–æ–∫–æ–≤ json
            cleaned_text = response_text.replace("```json", "").replace("```", "").strip()
            results = json.loads(cleaned_text)
            
            for res in results:
                if res.get("is_relevant"):
                    cat = res.get("category", "–î—Ä—É–≥–æ–µ")
                    if cat not in categories_data:
                        categories_data[cat] = []
                    categories_data[cat].append(res.get("text"))
                    
        except Exception as e:
            print(f"      ‚ö†Ô∏è Batch parsing error: {e}")
            
    # === –≠–¢–ê–ü 2: –ì–õ–£–ë–û–ö–ò–ô –ê–ù–ê–õ–ò–ó (Pro) ===
    print(f"   üß† Stage 2: Deep Analysis (Gemini Pro)...")
    print(f"      Found {len(categories_data)} categories.")
    
    top_pains = []
    
    for category, texts in categories_data.items():
        if len(texts) < 3: # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            continue
            
        print(f"      Analyzing category: {category} ({len(texts)} items)...")
        
        # –ë–µ—Ä–µ–º —Ç–æ–ø-50 —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
        analysis_text = "\n".join(texts[:50])
        
        prompt = f"""
        –¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –∏ –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å.
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ "{category}".
        
        –î–∞–Ω–Ω—ã–µ:
        {analysis_text}
        
        –¢–≤–æ—è –∑–∞–¥–∞—á–∞:
        1. –í—ã–¥–µ–ª–∏ —Å–∞–º—É—é –æ—Å—Ç—Ä—É—é –∏ –º–∞—Å—Å–æ–≤—É—é "–±–æ–ª—å" –≤ —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
        2. –û—Ü–µ–Ω–∏ —á–∞—Å—Ç–æ—Ç—É (–Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∂–∞–ª–æ–±).
        3. –ü—Ä–µ–¥–ª–æ–∂–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –±–∏–∑–Ω–µ—Å-–∏–¥–µ—é (MVP) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –±–æ–ª–∏ –≤ –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω–µ.
        4. –û—Ü–µ–Ω–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ (—Å–∫–æ–ª—å–∫–æ –ª—é–¥–∏ –≥–æ—Ç–æ–≤—ã –ø–ª–∞—Ç–∏—Ç—å).
        
        –í–µ—Ä–Ω–∏ –æ—Ç–≤–µ—Ç –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
        {{
            "category": "{category}",
            "frequency": {len(texts)},
            "pain_summary": "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≥–ª–∞–≤–Ω–æ–π –±–æ–ª–∏",
            "potential": "–ù–∏–∑–∫–∏–π/–°—Ä–µ–¥–Ω–∏–π/–í—ã—Å–æ–∫–∏–π",
            "price_hint": "–ü—Ä–∏–º–µ—Ä–Ω–∞—è —Ü–µ–Ω–∞ (—Å—É–º)",
            "business_idea": "–û–ø–∏—Å–∞–Ω–∏–µ –∏–¥–µ–∏ MVP",
            "examples": ["–ü—Ä–∏–º–µ—Ä 1", "–ü—Ä–∏–º–µ—Ä 2", "–ü—Ä–∏–º–µ—Ä 3"]
        }}
        """
        
        response_text = call_gemini(GEMINI_PRO_MODEL, prompt)
        
        try:
            cleaned_text = response_text.replace("```json", "").replace("```", "").strip()
            analysis = json.loads(cleaned_text)
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (—Ñ–µ–π–∫–æ–≤—ã–µ –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏, —Ç–∞–∫ –∫–∞–∫ –º—ã –∏—Ö –ø–æ—Ç–µ—Ä—è–ª–∏ –Ω–∞ —ç—Ç–∞–ø–µ 1 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤, 
            # –Ω–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ —Å—Ç–æ–∏—Ç –ø—Ä–æ–∫–∏–¥—ã–≤–∞—Ç—å)
            analysis["sources"] = ["Aggregated"] 
            top_pains.append(analysis)
        except Exception as e:
             print(f"      ‚ö†Ô∏è Category analysis error: {e}")

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —á–∞—Å—Ç–æ—Ç–µ
    top_pains.sort(key=lambda x: x["frequency"], reverse=True)
    return top_pains


def save_top_pains(pains: list):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–ø –±–æ–ª–µ–π –≤ Markdown"""
    output_file = BASE_DIR / "data" / f"top_pains_{TODAY}.md"
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(f"# üéØ –¢–æ–ø –ë–æ–ª–µ–π –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω–∞ ‚Äî {TODAY}\n\n")
        f.write(f"> –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ AI: {GEMINI_FLASH_MODEL} (Filter) + {GEMINI_PRO_MODEL} (Analysis)\n\n")
        
        for i, pain in enumerate(pains, 1):
            f.write(f"## {i}. {pain['category']}\n\n")
            f.write(f"- **–°—É—Ç—å –ø—Ä–æ–±–ª–µ–º—ã:** {pain['pain_summary']}\n")
            f.write(f"- **–ß–∞—Å—Ç–æ—Ç–∞:** {pain['frequency']} –∂–∞–ª–æ–±\n")
            f.write(f"- **–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª:** {pain['potential']}\n")
            f.write(f"- **–¶–µ–Ω–∞ —Ä–µ—à–µ–Ω–∏—è:** {pain['price_hint']}\n\n")
            
            f.write(f"### üí° –ë–∏–∑–Ω–µ—Å-–∏–¥–µ—è\n")
            f.write(f"{pain['business_idea']}\n\n")
            
            f.write(f"### –ü—Ä–∏–º–µ—Ä—ã –∂–∞–ª–æ–±\n")
            for example in pain.get('examples', []):
                f.write(f"- \"{example}\"\n")
            f.write("\n---\n\n")
    
    print(f"‚úÖ Saved top pains to {output_file}")
    return output_file


def run():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–≥–µ–Ω—Ç–∞"""
    print("üß† Pain Extractor Agent starting...")
    print(f"üìÖ Date: {TODAY}")
    print(f"ü§ñ Models: {GEMINI_FLASH_MODEL} + {GEMINI_PRO_MODEL}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    print("\nüìÇ Loading fresh data...")
    all_data = load_all_fresh_data()
    
    sources_loaded = sum(1 for v in all_data.values() if v)
    print(f"   Sources loaded: {sources_loaded}")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–æ–ª–∏
    print("\nüîç Extracting pain texts...")
    pain_texts = extract_pain_texts(all_data)
    print(f"   Pain texts found: {len(pain_texts)}")
    
    if not pain_texts:
        print("‚ö†Ô∏è No data found. Skipping analysis.")
        return []

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
    print("\nüß† Analyzing with AI Pipeline...")
    top_pains = analyze_pains_with_gemini(pain_texts)
    print(f"   Categories identified: {len(top_pains)}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    if top_pains:
        save_top_pains(top_pains)
        
        print(f"\nüìä Top 3 Pain Categories:")
        for pain in top_pains[:3]:
            print(f"   üî• {pain['category']}: {pain['frequency']} –∂–∞–ª–æ–±")
            print(f"      üí° {pain['business_idea'][:60]}...")
    
    return top_pains


if __name__ == "__main__":
    run()
